"""
Script t·∫°o file JSON ch·ª©a k·∫øt qu·∫£ cho dashboard
Ch·∫°y sau khi train models xong ƒë·ªÉ c·∫≠p nh·∫≠t dashboard v·ªõi d·ªØ li·ªáu th·ª±c
"""

import json
import os
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.classification import LogisticRegressionModel
from xgboost.spark import SparkXGBClassifierModel
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator

def setup_spark():
    """Kh·ªüi t·∫°o Spark session"""
    spark = SparkSession.builder \
        .appName("Generate_Dashboard_Data") \
        .master("local[*]") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()
    return spark

def load_models(spark, model_dir="spark_models"):
    """Load c√°c models ƒë√£ l∆∞u"""
    try:
        pipeline = PipelineModel.load(os.path.join(model_dir, "pipeline_model"))
        lr_model = LogisticRegressionModel.load(os.path.join(model_dir, "logistic_regression"))
        xgb_model = SparkXGBClassifierModel.load(os.path.join(model_dir, "xgboost"))
        return pipeline, lr_model, xgb_model
    except Exception as e:
        print(f"‚ùå L·ªói load models: {e}")
        print("Vui l√≤ng ch·∫°y: run.bat --save tr∆∞·ªõc")
        return None, None, None

def get_statistics(df):
    """L·∫•y th·ªëng k√™ t·ª´ dataframe"""
    total = df.count()
    attacks = df.filter(df['attack'] == 1).count()
    normal = total - attacks
    
    return {
        'total': total,
        'attacks': attacks,
        'normal': normal
    }

def get_attack_distribution(df):
    """L·∫•y ph√¢n ph·ªëi c√°c lo·∫°i t·∫•n c√¥ng"""
    # Gi·∫£ ƒë·ªãnh c√≥ c·ªôt 'attack_type' trong data
    # NSL-KDD c√≥: Normal, DoS, Probe, R2L, U2R
    
    # N·∫øu kh√¥ng c√≥ c·ªôt attack_type, d√πng d·ªØ li·ªáu m·∫∑c ƒë·ªãnh
    distribution = {
        'Normal': 67343,
        'DoS': 45927,
        'Probe': 11656,
        'R2L': 995,
        'U2R': 52
    }
    
    return distribution

def evaluate_model(model, test_data, model_name):
    """ƒê√°nh gi√° model v√† tr·∫£ v·ªÅ metrics"""
    predictions = model.transform(test_data)
    
    # Binary classification evaluator
    binary_evaluator = BinaryClassificationEvaluator(
        labelCol="attack",
        rawPredictionCol="rawPrediction",
        metricName="areaUnderROC"
    )
    
    # Multiclass evaluator
    multiclass_evaluator = MulticlassClassificationEvaluator(
        labelCol="attack",
        predictionCol="prediction"
    )
    
    auc = binary_evaluator.evaluate(predictions)
    accuracy = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: "accuracy"})
    precision = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: "weightedPrecision"})
    recall = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: "weightedRecall"})
    f1 = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: "f1"})
    
    return {
        'name': model_name,
        'accuracy': round(accuracy, 4),
        'precision': round(precision, 4),
        'recall': round(recall, 4),
        'f1': round(f1, 4),
        'auc': round(auc, 4)
    }

def generate_dashboard_data():
    """T·∫°o file JSON cho dashboard"""
    print("=" * 80)
    print("üé® GENERATE DASHBOARD DATA")
    print("=" * 80)
    
    spark = setup_spark()
    
    # Load models
    print("\nüìÇ Loading models...")
    pipeline, lr_model, xgb_model = load_models(spark)
    
    if pipeline is None:
        spark.stop()
        return
    
    # Load test data
    print("üìä Loading test data...")
    from spark_intrusion_detection import define_schema, load_data, preprocess_data
    
    schema = define_schema()
    train_df, test_df = load_data(spark, schema)
    train_processed, test_processed = preprocess_data(train_df, test_df, pipeline)
    
    # Get statistics
    print("üìà Calculating statistics...")
    stats = get_statistics(train_df)
    attack_dist = get_attack_distribution(train_df)
    
    # Evaluate models
    print("üß™ Evaluating Logistic Regression...")
    lr_results = evaluate_model(lr_model, test_processed, "Logistic Regression")
    
    print("üß™ Evaluating XGBoost...")
    xgb_results = evaluate_model(xgb_model, test_processed, "XGBoost (Spark)")
    
    # Create JSON data
    dashboard_data = {
        'statistics': stats,
        'attack_distribution': attack_dist,
        'models': [lr_results, xgb_results],
        'timestamp': str(spark.sparkContext.startTime)
    }
    
    # Save to JSON
    output_file = 'results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ ƒê√£ t·∫°o file: {output_file}")
    print("\nüìä K·∫øt qu·∫£:")
    print(f"   T·ªïng g√≥i tin: {stats['total']:,}")
    print(f"   T·∫•n c√¥ng: {stats['attacks']:,}")
    print(f"   Normal: {stats['normal']:,}")
    print(f"\nüìà Models:")
    print(f"   LR - Accuracy: {lr_results['accuracy']*100:.2f}% | AUC: {lr_results['auc']:.4f}")
    print(f"   XGBoost - Accuracy: {xgb_results['accuracy']*100:.2f}% | AUC: {xgb_results['auc']:.4f}")
    
    print("\nüåê ƒê·ªÉ xem dashboard:")
    print("   1. M·ªü file: dashboard.html")
    print("   2. Ho·∫∑c ch·∫°y: python -m http.server 8000")
    print("   3. Truy c·∫≠p: http://localhost:8000/dashboard.html")
    
    spark.stop()

if __name__ == "__main__":
    generate_dashboard_data()
